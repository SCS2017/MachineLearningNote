# 决策树

- 树与二叉树
- 基本流程
- 划分选择
- 剪枝处理
- 连续值与缺失值处理

## 树与二叉树

#### 树是有限元素的集合

- 根结点、内部节点、叶结点
- 父结点、子结点
- 层次、深度

![img](https://images0.cnblogs.com/blog/413416/201303/16224218-92e0f06926b443a9845653b27b2187d8.png)

#### 二叉树

- 每个结点最多只能有2个子结点

![img](https://images0.cnblogs.com/blog/413416/201303/17000135-75060e3ee81847c6892d2167710b4317.png)

### 简单实例

![选区_114](/home/scs/图片/选区_114.png)

![选区_115](/home/scs/图片/选区_115.png)

## 基本流程

![选区_097](/home/scs/图片/选区_097.png)

#### 决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回

- 当前结点包含的样本全属于同一类别，无需划分

- 当前属性为空，或是所有样本在所有属性上取值相同，无需划分

  把当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别

- 当前结点包含的样本集合为空，不能划分

  把当前结点标记为叶结点，将其类别设定为其父结点所含样本最多的类别

### 划分选择

- 信息增益        - >  ID3算法

  属性A对训练数据集D的信息增益Gain(D,A)定义为集合D的经验熵Ent(D)与属性A给定条件下D的经验条件熵Ent(D|A)之差

  表示划分前后不确定性减少的程度，应选择信息增益最大的属性对样本进行划分

$$
Gain(D,A)=Ent(D)-Ent(D|A)
$$

$$
Ent(D)=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|}
$$

$$
Ent(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}Ent(D_i)
$$



- 信息增益比    - >  C4.5算法

  以信息增益为划分指标，存在偏向选择取值较多的属性的问题

  属性A对训练数据集D的信息增益Gain(D,A)定义为其信息增益与训练数据集关于属性A的熵Ent(A)

$$
Gain\_ratio(D,A)=\frac{Gain(D,A)}{Ent(A)}
$$

$$
Ent(A)=-\sum_{i=1}^{V}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}
$$



- 基尼指数        - >  CART算法

  描述数据集D的纯度，表示一个随机选中的样本在子集中被分错的可能性，应选择基尼指数最小的进行划分

$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
$$

$$
Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2
$$

$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
$$

### 决策树模型——决策树的生成

- ID3算法
- C4.5算法
- CART算法

### 实例——以二分类为例

![选区_098](/home/scs/图片/选区_098.png)

#### 首先计算属性“色泽”的信息增益：

$$
Ent(D)=-(\frac{8}{17}log_2\frac{8}{17}+\frac{9}{17}log_2\frac{9}{17})=0.998
$$

$$
Ent(D_1)=-(\frac{3}{6}log_2(\frac{3}{6})+\frac{3}{6}log_2(\frac{3}{6}))=1.000
$$

$$
Ent(D_2)=-(\frac{4}{6}log_2(\frac{4}{6})+\frac{2}{6}log_2(\frac{2}{6}))=0.918
$$

$$
Ent(D_3)=-(\frac{1}{5}log_2(\frac{1}{5})+\frac{4}{5}log_2(\frac{4}{5}))=0.722
$$

$$
Gain(D,色泽)=Ent(D)-[\frac{6}{17}Ent(D_1)+\frac{6}{17}Ent(D_2)+\frac{5}{17}Ent(D_3)]=0.109
$$

#### 类似的，可计算其他属性的信息增益

$$
Gain(D,根蒂)=0.143;Gain(D,敲声)=0.141;Gain(D,纹理)=0.381;
$$

$$
Gain(D,脐部)=0.381;Gain(D,触感)=0.006;
$$

#### 显然，属性“纹理”的信息增益最大，于是被选为划分属性。划分结果如下：

![选区_100](/home/scs/图片/选区_100.png)

#### 接下来进行进一步划分，最终结果如图

![选区_099](/home/scs/图片/选区_099.png)



### 剪枝处理

- 预剪枝     训练时间开销小，但有欠拟合的风险

  在决策树生成过程中，对每个结点在划分之前先进行估计，若当前结点的划分不能带来泛化性能提升，则停止划分并将当前结点标记为叶结点

- 后剪枝     泛化性能更好，但训练时间开销大

  先生成一颗完整的决策树，然后自底向上的对叶结点进行考察，若将该结点对应的子树替换为叶结点带来泛化性能的提升，则将该结点替换为叶结点

#### 举例来说，首先将西瓜数据集分成训练集和验证集

![选区_103](/home/scs/图片/选区_103.png)

#### 生成的未剪枝的决策树如下

![选区_108](/home/scs/图片/选区_108.png)

#### 预剪枝

![选区_109](/home/scs/图片/选区_109.png)

#### 后剪枝

![选区_110](/home/scs/图片/选区_110.png)

### 连续值与缺失值处理

- 连续值

  取每个区间的中点，找到其中信息增益最大的点，作为划分点。

  对连续属性A，可考察包含n-1个元素的候选划分点集合：
  $$
  T_a=\{\frac{a^i+a^{i+1}}{2} | 1≤i≤n-1\}
  $$
  然后选取最优的划分点进行样本集合划分：
  $$
  Gain(D,A)=\max_{t \in T_a}Gain(D,A,t)=\max_{t \in T_a}Ent(D)-\sum_{\lambda\in\{-,+\}}\frac{|D_t^\lambda|}{|D|}Ent(D_t^\lambda)
  $$




![选区_111](/home/scs/图片/选区_111.png)

- 缺失值

  如果仅对无缺失值的样本进行学习，显然是对数据信息极大的浪费。

  两个问题：

  - 如何在属性值缺失的情况下进行划分属性选择？
  - 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

  为每个样本x赋予一个权重w_x,并定义
  $$
  \rho=\frac{\sum_{x\in \tilde{D}}w_x}{\sum_{x\in D}w_x}
  $$

  $$
  \tilde{r_v}=\frac{\sum_{x\in \tilde{D}^v}w_x}{\sum_{x\in \tilde{D}}w_x}
  $$

  这样就可将信息增益的公式推广为：
  $$
  Gain(D,A)=\rho×Gain(\tilde{D},A)=\rho×(Ent(\tilde{D})-\sum_{k=1}^{K}\tilde{r}^vEnt(\tilde{D}))
  $$
  其中
  $$
  Ent(\tilde{D}）=-\sum_{k=1}^{K}\tilde{p_k}log_2\tilde{p_k}
  $$




![选区_112](/home/scs/图片/选区_112.png)

![选区_113](/home/scs/图片/选区_113.png)