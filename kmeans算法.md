#### 传统K-Means算法流程

在上一节我们对K-Means的原理做了初步的探讨，这里我们对K-Means的算法做一个总结。

首先我们看看K-Means算法的一些要点。

　1）对于K-Means算法，首先要注意的是k值的选择，一般来说，我们会根据对数据的先验经验选择一个合适的k值，如果没有什么先验知识，则可以通过交叉验证选择一个合适的k值。

　2）在确定了k的个数后，我们需要选择k个初始化的质心，就像上图b中的随机质心。由于我们是启发式方法，k个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的k个质心，最好这些质心不能太近。

好了，现在我们来总结下传统的K-Means算法流程。　

输入是样本集,聚类的簇树k,最大迭代次数N

输出是簇划分　

　1) 从数据集D中随机选择k个样本作为初始的k个质心向量： 

　2）对于n=1,2,...,N

　　　a) 将簇划分C初始化为

　　　b) 对于i=1,2...m,计算样本和各个质心向量的距离：，将标记最小的为所对应的类别。此时更新

　　　c) 对于j=1,2,...,k,对中所有的样本点重新计算新的质心

　　　e) 如果所有的k个质心向量都没有发生变化，则转到步骤3）

　3） 输出簇划分

#### K-Means初始化优化K-Means++

在上节我们提到，k个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的k个质心。如果仅仅是完全随机的选择，有可能导致算法收敛很慢。K-Means++算法就是对K-Means随机初始化质心的方法的优化。

K-Means++的对于初始化质心的优化策略也很简单，如下：

　a)  从输入的数据点集合中随机选择一个点作为第一个聚类中心

　b) 对于数据集中的每一个点，计算它与已选择的聚类中心中最近聚类中心的距离

　c) 选择一个新的数据点作为新的聚类中心，选择的原则是：距离较大的点，被选取作为聚类中心的概率较大

　d) 重复b和c直到选择出k个聚类质心

　e) 利用这k个质心来作为初始化质心去运行标准的K-Means算法

#### K-Means小结

K-Means是个简单实用的聚类算法，这里对K-Means的优缺点做一个总结。

K-Means的主要优点有：

　1）原理比较简单，实现也是很容易，收敛速度快。

　2）聚类效果较优。

　3）算法的可解释度比较强。

　4）主要需要调参的参数仅仅是簇数k。

K-Means的主要缺点有：

　1）K值的选取不好把握(改进：可以通过在一开始给定一个适合的数值给k，通过一次K-means算法得到一次聚类中心。对于得到的聚类中心，根据得到的k个聚类的距离情况，合并距离最近的类，因此聚类中心数减小，当将其用于下次聚类时，相应的聚类数目也减小了，最终得到合适数目的聚类数。可以通过一个评判值E来确定聚类数得到一个合适的位置停下来，而不继续合并聚类中心。重复上述循环，直至评判函数收敛为止，最终得到较优聚类数的聚类结果)。

　2）对于不是凸的数据集比较难收敛(改进：基于密度的聚类算法更加适合，比如DESCAN算法)

　3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。

　4） 采用迭代方法，得到的结果只是局部最优。

　5） 对噪音和异常点比较的敏感(改进1：离群点检测的LOF算法，通过去除离群点后再聚类，可以减少离群点和孤立点对于聚类效果的影响；改进2：改成求点的中位数，这种聚类方式即K-Mediods聚类（K中值）)。

​    6）初始聚类中心的选择(改进1：k-means++;改进2：二分K-means，相关知识详见[这里](http://blog.csdn.net/gamer_gyt/article/details/48949227)和[这里](http://blog.csdn.net/zouxy09/article/details/17590137))。