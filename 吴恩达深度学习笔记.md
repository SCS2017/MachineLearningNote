#### 第二门课 改善深层神经网络：超参数调试、正则化以及优化

[第一周：深度学习的实用层面(Practical aspects of Deep Learning)](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n3)

[1.1 训练，验证，测试集（Train / Dev / Test sets）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n5)

[1.2 偏差，方差（Bias /Variance）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n77)

[1.3 机器学习基础（Basic Recipe for Machine Learning）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n144)

[1.4 正则化（Regularization）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n165)

[1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n224)

[1.6 dropout 正则化（Dropout Regularization）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n279)

[1.7 理解 dropout（Understanding Dropout）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n349)

[1.8 其他正则化方法（Other regularization methods）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n380)

[1.9 归一化输入（Normalizing inputs）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n435)

[1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n482)

[1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing / Exploding gradients）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n507)

[1.12 梯度的数值逼近（Numerical approximation of gradients）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n531)

[1.13 梯度检验（Gradient checking）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n554)

[1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）](http://www.ai-start.com/dl2017/html/lesson2-week1.html#header-n589)

**1.1 训练，验证，测试集（Train / Dev / Test sets）**

应用深度学习是一个典型的迭代过程，需要多次循环往复，才能为应用程序找到一个称心的神经网络，因此循环该过程的效率是决定该项目进展速度的一个关键因素，而创造高质量的训练数据集，验证集和测试集也有助于提高循环效率。

在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是人们常说的70%验证集，30%测试集，如果没有明确设置验证集，也可以按照60%训练，20%验证和20%测试集来划分。这是前几年机器学习领域普遍认可的最好的实践方法。

但是在大数据时代，我们现在的数据量可能是百万级别，那么验证集和测试集占数据总量的比例会趋向于变得更小。假设我们有100万条数据，其中1万条作为验证集，1万条作为测试集，100万里取1万，比例是1%，即：训练集占98%，验证集和测试集各占1%。对于数据量过百万的应用，训练集可以占到99.5%，验证和测试集各占0.25%，或者验证集占0.4%，测试集占0.1%。

验证集的目的就是验证不同的算法，检验哪种算法更有效。测试集的主要目的是正确评估分类器的性能。可以只有训练集和验证集，没有测试集。

**1.2 偏差，方差（Bias /Variance）**

以下分析的前提都是假设基本误差很小，训练集和验证集数据来自相同分布。

高偏差对应欠拟合

高方差对应过拟合

理解偏差和方差的两个关键数据是训练集误差（Train set error）和验证集误差（Dev set error）。

假定训练集误差是1%，验证集误差是11%，像这种情况，我们称之为“高方差”。

假设训练集误差是15%，验证集误差是16%，这种算法偏差高，因为它甚至不能拟合训练集。

![img](http://www.ai-start.com/dl2017/images/c61d149beecddb96f0f93944320cf639.png)

**1.3 机器学习基础（Basic Recipe for Machine Learning）**

有两点需要大家注意：

第一点，高偏差和高方差是两种不同的情况，我们后续要尝试的方法也可能完全不同，我通常会用训练验证集来诊断算法是否存在偏差或方差问题，然后根据结果选择尝试部分方法。

第二点，在机器学习的初期阶段，关于所谓的偏差方差权衡的讨论屡见不鲜，原因是我们能尝试的方法有很多。可以增加偏差，减少方差，也可以减少偏差，增加方差，但是在深度学习的早期阶段，我们没有太多工具可以做到只减少偏差或方差却不影响到另一方。但在当前的深度学习和大数据时代，只要持续训练一个更大的网络，只要准备了更多数据，那么也并非只有这两种情况，我们假定是这样，那么，只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。这两步实际要做的工作是：训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。我觉得这就是深度学习对监督式学习大有裨益的一个重要原因，也是我们不用太过关注如何平衡偏差和方差的一个重要原因，但有时我们有很多选择，减少偏差或方差而不增加另一方。最终，我们会得到一个非常规范化的网络。

机器学习中存在偏差方差的权衡问题。而深度学习，大量可利用的数据，以及可训练的更深的网络，是的我们可以在减小偏差和方差一方的同时，不会过多地影响另一方。

**1.4 正则化（Regularization）**

深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。

L1正则化使模型变得稀疏，减小了过拟合，同时也进行了特征选择的过程。

L2正则化则使模型的参数更小，一般认为，参数较小的模型⽐较简单，能适应不同的数据集，也在⼀定程度上避免了过拟合现象。

**1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）**

![img](http://www.ai-start.com/dl2017/images/d5ee6f2b60ff7601d50967f4365d0ecb.png)

以神经网络为例，

直观上理解如果正则化参数设置的足够大，权重被设置为接近于0的值，相当于某些连接不存在，简化了神经网络，可是深度却很大，会使这个网络从过拟合的状态变成高偏差的状态。但是会有一个中间值，一个“Just Right”的状态。

**1.6 dropout 正则化（Dropout Regularization）**

![img](http://www.ai-start.com/dl2017/images/97e37bf0d2893f890561cda932ba8c42.png)

假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用backprop方法进行训练。

假设keep-prob=0.8，则代表去掉20%的神经元。

训练阶段为了保证某一层输出的期望值不变，需要将保留神经元的权重和输入的乘积和除以0.8。

此外，测试阶段不需要进行dropout。

**1.7 理解 dropout（Understanding Dropout）**

![img](http://www.ai-start.com/dl2017/images/L2_week1_16.png)

直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；L2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。

如果你担心某些层比其它层更容易发生过拟合，可以把某些层的keep-prob值设置得比其它层更低，缺点是为了使用交叉验证，你要搜索更多的超级参数，另一种方案是在一些层上应用dropout，而有些层不用dropout，应用dropout的层只含有一个超级参数，就是keep-prob。

要牢记一点，dropout是一种正则化方法，它有助于预防过拟合，因此除非算法过拟合，不然我是不会使用dropout的，所以它在其它领域应用得比较少，主要存在于计算机视觉领域。

dropout的一大缺点就是代价函数J不在被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。

**1.8 其他正则化方法（Other regularization methods）**

（1）数据扩增  增大数据集

比如对于图片来说，可以翻转图片和裁剪图片。

（2）早停

![img](http://www.ai-start.com/dl2017/images/9d0db64a9c9b050466a039c935f36f93.png)



**1.9 归一化输入（Normalizing inputs）**

训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤：

（1）零均值

（2）归一化方差；

![img](http://www.ai-start.com/dl2017/images/4d0c183882a140ecd205f1618243d7f8.png)

归一化是的参数学习的过程更加简单快速。



**1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）**

权重参数大于1时，经过很多层，以指数级增长，会导致梯度爆炸。1.5的N次方

权重参数小于1时，经过很多层，以指数级增长，会导致梯度消失。0.5的N次方



**1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing / Exploding gradients）**

输入特征被零均值和标准方差化，方差是1，确实降低了梯度消失和爆炸问题。



**1.12 梯度的数值逼近（Numerical approximation of gradients）**

简单来说就是导数的定义。

![img](http://www.ai-start.com/dl2017/images/1b2617461ed10089bc61ad273c84594f.png)



**1.13 梯度检验（Gradient checking）**

![img](http://www.ai-start.com/dl2017/images/f56f0cdc90730068109a6e20fcd41724.png)

对参数\theta增加\epsilon，其他项保持不变。



**1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）**

![img](http://www.ai-start.com/dl2017/images/L2_week1_24.png)



#### 第二周：优化算法 (Optimization algorithms)

[第二周：优化算法 (Optimization algorithms)](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n0)

[2.1 Mini-batch 梯度下降（Mini-batch gradient descent）](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n4)

[2.2 理解mini-batch梯度下降法（Understanding mini-batch gradient descent）](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n57)

[2.3 指数加权平均数（Exponentially weighted averages）](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n95)

[2.4 理解指数加权平均数（Understanding exponentially weighted averages）](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n149)

[2.5 指数加权平均的偏差修正（Bias correction in exponentially weighted averages）](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n224)

[2.6 动量梯度下降法（Gradient descent with Momentum）](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n245)

[2.7 RMSprop](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n281)

[2.8 Adam 优化算法(Adam optimization algorithm)](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n311)

[2.9 学习率衰减(Learning rate decay)](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n348)

[2.10 局部最优的问题(The problem of local optima)](http://www.ai-start.com/dl2017/html/lesson2-week2.html#header-n385)



**2.1 Mini-batch 梯度下降（Mini-batch gradient descent）**

假设有500万个样本，batch_size=1000，也就是每次训练1000个样本，总共有5000个mini_batch。

5000次迭代遍历一遍数据集中的所有样本，也是一个epoch。



**2.2 理解mini-batch梯度下降法（Understanding mini-batch gradient descent）**

![img](http://www.ai-start.com/dl2017/images/b5c07d7dec7e54bed73cdcd43e79452d.png)

使用mini-batch梯度下降法，损失函数则并不是每次迭代都是下降的，而是上下波动的。

你需要决定的变量之一是mini-batch的大小，m是训练集的大小，极端情况下，如果mini-batch的大小等于m，其实就是batch梯度下降法。

另一种极端情况，假设mini-batch大小为1，就有了新的算法，叫做随机梯度下降法，每个样本都是独立的mini-batch。

小于2000个样本，这样比较适合使用batch梯度下降法。不然，样本数目较大的话，一般的mini-batch大小为64到512，考虑到电脑内存设置和使用的方式。



**2.3 指数加权平均数（Exponentially weighted averages）**

好像MACD和KDJ这些指标的计算就用到了这种方法。

一种计算趋势的方法

在统计学中被称为指数加权移动平均值。相当于一定长度的滑窗的加权平均值。

![img](http://www.ai-start.com/dl2017/images/358269d36d06e3c70da3e87d8dc523e4.png)

偏差修正

在估计的初期，估计值往往远小于初始值。

![img](http://www.ai-start.com/dl2017/images/26a3c3022a7f7ae7ba0cd27fc74cbcf6.png)

  

![img](http://www.ai-start.com/dl2017/images/f602c9d517a7f6c01fe18171dade17e6.png)



![截图](/home/scs/桌面/截图.png)

**2.6 动量梯度下降法（Gradient descent with Momentum）**

动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法，简而言之，基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新你的权重。

![img](http://www.ai-start.com/dl2017/images/cc2d415b8ccda9fdaba12c575d4d3c4b.png)

这种上下波动减慢了梯度下降法的速度，你就无法使用更大的学习率，如果你要用较大的学习率（紫色箭头），结果可能会偏离函数的范围，为了避免摆动过大，你要用一个较小的学习率。

在纵轴上，我们希望学习慢一点，因为我们不想要这些摆动，但是在横轴上，我们希望加快学习，你希望快速从左向右移，移向最小值，移向红点。

![img](http://www.ai-start.com/dl2017/images/a3af0fb5e49e16c108e72d015ef0fdb6.png)

同样的，Vdb和Vdw的初始值为0向量，也可以使用偏差修正。算法中用到了参数w和b的微分，平滑的是速度。

两个超参数α和β，α是学习率，β控制着指数加权平均数，β最常用的值是0.9，即平均了前十次迭代的梯度。



**2.7 RMSprop**

全称是root mean square prop算法，它也可以加速梯度下降，我们来看看它是如何运作的。

![img](http://www.ai-start.com/dl2017/images/553ee26f6efd82d9996dec5f77e3f12e.png)

算法中采用了参数w和b的导数的平方

可以使用更大的学习率，来保证在纵轴方向偏离较小的同时，横轴方向移动较快。

![截图1](/home/scs/桌面/截图1.png)

**2.8 Adam 优化算法(Adam optimization algorithm)**

Adam代表的是Adaptive Moment Estimation，结合了Momentum和RMSprop

![img](http://www.ai-start.com/dl2017/images/9ca9bfc160d53b23ea0d1164e6accffe.png)



![img](http://www.ai-start.com/dl2017/images/e9858303cd62eacc21759b16a121ff58.png)



**2.9 学习率衰减(Learning rate decay)**

加快学习算法的一个办法就是随时间慢慢减少学习率，我们将之称为学习率衰减，我们来看看如何做到，首先通过一个例子看看，为什么要计算学习率衰减。

慢慢减少学习率的本质在于，在学习初期，你能承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。

![img](http://www.ai-start.com/dl2017/images/7e0edfb697e8262dc39a040a987c62bd.png)

指数衰减

![img](http://www.ai-start.com/dl2017/images/e1b6dc57b8b73ecf5ff400852c4f7086.png)



**2.10 局部最优的问题(The problem of local optima)**

在深度学习研究早期，人们总是担心优化算法会困在极差的局部最优，不过随着深度学习理论不断发展，我们对局部最优的理解也发生了改变。我向你展示一下现在我们怎么看待局部最优以及深度学习中的优化问题。

![img](http://www.ai-start.com/dl2017/images/1f7df04b804836fbcadcd258c0b55f74.png)

事实上，如果你要创建一个神经网络，通常梯度为零的点并不是这个图中的局部最优点，实际上成本函数的零梯度点，通常是鞍点。而不是局部最优点。

![img](http://www.ai-start.com/dl2017/images/a8c3dfdc238762a9f0edf26e6037ee09.png)

至于为什么会把一个曲面叫做鞍点，你想象一下，就像是放在马背上的马鞍一样，如果这是马，这是马的头，这就是马的眼睛，画得不好请多包涵，然后你就是骑马的人，要坐在马鞍上，因此这里的这个点，导数为0的点，这个点叫做鞍点。我想那确实是你坐在马鞍上的那个点，而这里导数为0。

![img](http://www.ai-start.com/dl2017/images/011e9625870797d6c0695658b92f606e.png)

所以此次视频的要点是，首先，你不太可能困在极差的局部最优中，条件是你在训练较大的神经网络，存在大量参数，并且成本函数J被定义在较高的维度空间。

第二点，平稳段是一个问题，这样使得学习十分缓慢，这也是像Momentum或是RMSprop，Adam这样的算法，能够加速学习算法的地方。在这些情况下，更成熟的优化算法，如Adam算法，能够加快速度，让你尽早往下走出平稳段。



#### 第三周 超参数调试、Batch正则化和程序框架（Hyperparameter tuning）

[3.1 调试处理（Tuning process）](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n4)

[3.2 为超参数选择合适的范围（Using an appropriate scale to pick hyperparameters）](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n45)

[3.3 超参数训练的实践：Pandas VS Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n88)

[3.4 归一化网络的激活函数（Normalizing activations in a network）](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n119)[3.5 将 Batch Norm 拟合进神经网络（Fitting Batch Norm into a neural network）](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n160)

[3.6 Batch Norm 为什么奏效？（Why does Batch Norm work?）](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n219)

[3.7 测试时的 Batch Norm（Batch Norm at test time）](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n276)

[3.8 Softmax 回归（Softmax regression）](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n297)

[3.9 训练一个 Softmax 分类器（Training a Softmax classifier）](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n342)

[3.10 深度学习框架（Deep Learning frameworks）](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n398)

[3.11 TensorFlow](http://www.ai-start.com/dl2017/html/lesson2-week3.html#header-n415)

**3.1 调试处理（Tuning process）**

![img](http://www.ai-start.com/dl2017/images/7b73f4de29ea13d9aba5f49e393d4674.png)

结果证实一些超参数比其它的更为重要，a无疑是最重要的，接下来是我用橙色圈住的那些，然后是我用紫色圈住的那些，但这不是严格且快速的标准。

![img](http://www.ai-start.com/dl2017/images/75bfa084ea64d99b1d01a393a7c988a6.png)

在早一代的机器学习算法中，如果你有两个超参数，这里我会称之为超参1，超参2，常见的做法是在网格中取样点，像这样，然后系统的研究这些数值。这里我放置的是5×5的网格，实践证明，网格可以是5×5，也可多可少，但对于这个例子，你可以尝试这所有的25个点，然后选择哪个参数效果最好。当参数的数量相对较少时，这个方法很实用。

在深度学习领域，我推荐你采用下面的做法，随机选择点，所以你可以选择同等数量的点，对吗？25个点，接着，用这些随机取的点试验超参数的效果。之所以这么做是因为，对于你要解决的问题而言，你很难提前知道哪个超参数最重要，正如你之前看到的，一些超参数的确要比其它的更重要。

当你给超参数取值时，另一个惯例是采用由粗糙到精细的策略。

![img](http://www.ai-start.com/dl2017/images/c3b248ac8ca2cf646d5b705270e01e78.png)



**3.2 为超参数选择合适的范围（Using an appropriate scale to pick hyperparameters）**

比如选择网络层数或是神经元的节点个数，均匀选择就可以。

![img](http://www.ai-start.com/dl2017/images/3b45624120e77aea2fcd117cbfdc9bdb.png)

但是学习率就不能这样了，取值为0.0001到1

![img](http://www.ai-start.com/dl2017/images/651422f74439fd7e648e364d26d21485.png)

于是我们可以取对数，r的范围是[-4，0]，而对应学习率a的范围是[0.0001，1]

![img](http://www.ai-start.com/dl2017/images/a54d5ea6cd623f741f75e62195f072ca.png)

另一个棘手的例子是给β取值，用于计算指数的加权平均值。假设你认为是0.9到0.999之间的某个值，也许这就是你想搜索的范围。记住这一点，当计算指数的加权平均值时，取0.9就像在10个值中计算平均值，有点类似于计算10天的温度平均值，而取0.999就是在1000个值中取平均。

![img](http://www.ai-start.com/dl2017/images/2c4e6d3419beb66ed0163331443c6b40.png)

与上面的方法类似，现在我们考虑1-β的取值，在0.1到0.001区间内

![img](http://www.ai-start.com/dl2017/images/2e3b1803ab468a94a4cae13e89217704.png)



**3.3 超参数训练的实践：Pandas VS Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）**

深度学习领域中，发展很好的一点是，不同应用领域的人们会阅读越来越多其它研究领域的文章，跨领域去寻找灵感。

所以这两种方式的选择，是由你拥有的计算资源决定的，如果你拥有足够的计算机去平行试验许多模型，那绝对采用鱼子酱方式，尝试许多不同的超参数，看效果怎么样。但在一些应用领域，比如在线广告设置和计算机视觉应用领域，那里的数据太多了，你需要试验大量的模型，所以同时试验大量的模型是很困难的，它的确是依赖于应用的过程。但我看到那些应用熊猫方式多一些的组织，那里，你会像对婴儿一样照看一个模型，调试参数，试着让它工作运转。



**3.4 归一化网络的激活函数（Normalizing activations in a network）**

在深度学习兴起后，最重要的一个思想是它的一种算法，叫做Batch Normalization，由Sergey loffe和Christian Szegedy两位研究者创造。Batch Normalization会使你的参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更加庞大，工作效果也很好，也会是你的训练更加容易，甚至是深层网络。

![img](http://www.ai-start.com/dl2017/images/7eed1a2ef94832c54d1765731a57b2b5.png)

当训练一个模型，比如logistic回归时，你也许会记得，归一化输入特征可以加快学习过程。你计算了平均值，从训练集中减去平均值，计算了方差，接着根据方差归一化你的数据集，在之前的视频中我们看到，这是如何把学习问题的轮廓，从很长的东西，变成更圆的东西，更易于算法优化。所以这是有效的，对logistic回归和神经网络的归一化输入特征值而言。

BN算法（Batch Normalization）其强大之处如下：

(1)你可以选择比较大的初始学习率，让你的训练速度飙涨。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；

(2)你再也不用去理会过拟合中dropout、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；

(3)再也不需要使用使用局部响应归一化层了（局部响应归一化是Alexnet网络用到的方法，搞视觉的估计比较熟悉），因为BN本身就是一个归一化网络层；

(4)可以把训练数据彻底打乱（防止每批训练的时候，某一个样本都经常被挑选到，文献说这个可以提高1%的精度，这句话我也是百思不得其解啊）。



Motivation

网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，因此我们不得不降低学习率、小心地初始化。网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度作者将分布发生变化称之为 internal covariate shift。

我们一般在训练网络的时会将输入减去均值，还有些人甚至会对输入做白化等操作，目的是为了加快训练。白化的方式有好几种，常用的有PCA白化：即对数据进行PCA操作之后，在进行方差归一化。这样数据基本满足0均值、单位方差、弱相关性。作者首先考虑，对每一层数据都使用白化操作，但分析认为这是不可取的。因为白化需要计算协方差矩阵、求逆等操作，计算量很大，此外，反向传播时，白化操作不一定可导。于是，作者采用下面的Normalization方法。

BN算法

数据归一化的方法很简单，

![20160223160039062](/home/scs/图片/20160223160039062.png)

但是作者又说如果简单的这么干，会降低层的表达能力。比如下图，在使用sigmoid激活函数的时候，如果把数据限制到0均值单位方差，那么相当于只使用了激活函数中近似线性的部分，这显然会降低模型表达能力。 

![20160223160053859](/home/scs/图片/20160223160053859.png)

为此，作者又为BN增加了2个参数，用来保持模型的表达能力，作者引入了两个可学习参数γ和β，通过学习可以恢复出原始网络所要学习的特征分布。

于是最后的输出为： 

![20160223160123115](/home/scs/图片/20160223160123115.png)

 

上述公式中用到了均值E和方差Var，需要注意的是理想情况下E和Var应该是针对整个数据集的，但显然这是不现实的。因此，作者做了简化，用一个Batch的均值和方差作为对整个数据集均值和方差的估计。 

整个BN算法如下

![20160223160132599](/home/scs/图片/20160223160132599.png)

文献主要是把BN变换，置于网络激活函数层的前面。在没有采用BN的时候，激活函数层是这样的：z=g(Wu+b)

也就是我们希望一个激活函数，比如s型函数s(x)的自变量x是经过BN处理后的结果。因此前向传导的计算公式就应该是：z=g(BN(Wu+b))

其实因为偏置参数b经过BN层后其实是没有用的，最后也会被均值归一化，当然BN层后面还有个β参数作为偏置项，所以b这个参数就可以不用了。因此最后把BN层+激活函数层就变成了：z=g(BN(Wu))

测试

测试时依然用到下面的式子

![20160223160145084](/home/scs/图片/20160223160145084.png)

特别注意： 这里的均值和方差已经不是针对某一个Batch了，而是针对整个数据集而言。因此，在训练过程中除了正常的前向传播和反向求导之外，我们还要记录每一个Batch的均值和方差，以便训练完成之后按照下式计算整体的均值和方差： 

![20160223160200068](/home/scs/图片/20160223160200068.png)



#### 第二周 深度卷积网络：实例探究（Deep convolutional models: case studies）

[第二周 深度卷积网络：实例探究（Deep convolutional models: case studies）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n0)

[2.1 为什么要进行实例探究？（Why look at case studies?）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n4)

[2.2 经典网络（Classic networks）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n19)[2.3 残差网络（Residual Networks (ResNets)）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n96)

[2.4 残差网络为什么有用？（Why ResNets work?）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n132)

[2.5 网络中的网络以及 1×1 卷积（Network in Network and 1×1 convolutions）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n178)

[2.6 谷歌 Inception 网络简介（Inception network motivation）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n220)

[2.7 Inception 网络（Inception network）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n280)

[2.8 使用开源的实现方案（Using open-source implementations）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n332)

[2.9 迁移学习（Transfer Learning）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n372)

[2.10 数据扩充（Data augmentation）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n410)

[2.11 计算机视觉现状（The state of computer vision）](http://www.ai-start.com/dl2017/html/lesson4-week2.html#header-n457)



**2.2 经典网络（Classic networks）**

这节课，我们来学习几个经典的神经网络结构，分别是LeNet-5、AlexNet和VGGNet，开始吧。

LeNet-5的网络结构





**2.3 残差网络（Residual Networks (ResNets)）**

非常非常深的神经网络是很难训练的，因为存在梯度消失和梯度爆炸问题。这节课我们学习跳跃连接（Skip connection），它可以从某一层网络层获取激活，然后迅速反馈给另外一层，甚至是神经网络的更深层。我们可以利用跳跃连接构建能够训练深度网络的ResNets，有时深度能够超过100层，让我们开始吧。

ResNets是由残差块（Residual block）构建的，首先我解释一下什么是残差块。

![img](http://www.ai-start.com/dl2017/images/4486bc7028169ae755a01af9c77204f6.png)

加入残差后，加法操作在激活函数ReLU之前。

![img](http://www.ai-start.com/dl2017/images/f0a8471f869d8062ba59598c418da7fb.png)



![img](http://www.ai-start.com/dl2017/images/6077958a616425d76284cecb43c2f458.png)

如果我们使用标准优化算法训练一个普通网络，比如说梯度下降法，或者其它热门的优化算法。如果没有残差，没有这些捷径或者跳跃连接，凭经验你会发现随着网络深度的加深，训练错误会先减少，然后增多。而理论上，随着网络深度的加深，应该训练得越来越好才对。也就是说，理论上网络深度越深越好。但实际上，如果没有残差网络，对于一个普通网络来说，深度越深意味着用优化算法越难训练。实际上，随着网络深度的加深，训练错误会越来越多。

但有了ResNets就不一样了，即使网络再深，训练的表现却不错，比如说训练误差减少，就算是训练深达100层的网络也不例外。有人甚至在1000多层的神经网络中做过实验，尽管目前我还没有看到太多实际应用。但是对的激活，或者这些中间的激活能够到达网络的更深层。这种方式确实有助于解决梯度消失和梯度爆炸问题，让我们在训练更深网络的同时，又能保证良好的性能。也许从另外一个角度来看，随着网络越来深，网络连接会变得臃肿，但是ResNet确实在训练深度网络方面非常有效。



**2.4 残差网络为什么有用？（Why ResNets work?）**

![img](http://www.ai-start.com/dl2017/images/f60f5ca514d4bad1288fc7cbd666dd99.png)

结果表明，残差块学习这个恒等式函数并不难，跳跃连接使我们很容易得出a(l+2)=a(l)。这意味着，即使给神经网络增加了这两层，它的效率也并不逊色于更简单的神经网络，因为学习恒等函数对它来说很简单。尽管它多了两层，也只把a(l)的值赋值给a(l+2)。所以给大型神经网络增加两层，不论是把残差块添加到神经网络的中间还是末端位置，都不会影响网络的表现。

残差网络起作用的主要原因就是这些残差块学习恒等函数非常容易，你能确定网络性能不会受到影响，很多时候甚至可以提高效率，或者说至少不会降低网络的效率，因此创建类似残差网络可以提升网络性能。



**2.5 网络中的网络以及 1×1 卷积（Network in Network and 1×1 convolutions）**

在架构内容设计方面，其中一个比较有帮助的想法是使用1×1卷积。也许你会好奇，1×1的卷积能做什么呢？不就是乘以数字么？听上去挺好笑的，结果并非如此，我们来具体看看。

过滤器为1×1，这里是数字2，输入一张6×6×1的图片，然后对它做卷积，起过滤器大小为1×1×1，结果相当于把这个图片乘以数字2，所以前三个单元格分别是2、4、6等等。用1×1的过滤器进行卷积，似乎用处不大，只是对输入矩阵乘以某个数字。但这仅仅是对于6×6×1的一个通道图片来说，1×1卷积效果不佳。

![img](http://www.ai-start.com/dl2017/images/84c62c1d9bdf0a14f20cbeb02e1cec1a.png)

如果是一张6×6×32的图片，那么使用1×1过滤器进行卷积效果更好。具体来说，1×1卷积所实现的功能是遍历这36个单元格，计算左图中32个数字和过滤器中32个数字的元素积之和，然后应用ReLU非线性函数。

![img](http://www.ai-start.com/dl2017/images/70eba35d0705dc681c40f09a0926061a.png)



所以1×1卷积可以从根本上理解为对这32个不同的位置都应用一个全连接层，全连接层的作用是输入32个数字,输出结果是6×6×#filters（过滤器数量），以便在输入层上实施一个非平凡（non-trivial）计算。



举个1×1卷积的例子，相信对大家有所帮助，这是它的一个应用。

假设这是一个28×28×192的输入层，你可以使用池化层压缩它的高度和宽度，这个过程我们很清楚。但如果通道数量很大，该如何把它压缩为28×28×32维度的层呢？你可以用32个大小为1×1的过滤器，严格来讲每个过滤器大小都是1×1×192维，因为过滤器中通道数量必须与输入层中通道的数量保持一致。但是你使用了32个过滤器，输出层为28×28×32，这就是压缩通道数（nc）的方法，对于池化层我只是压缩了这些层的高度和宽度。

![img](http://www.ai-start.com/dl2017/images/49a16fdc10769a86355911f9e324c728.png)

1×1卷积层就是这样实现了一些重要功能的（doing something pretty non-trivial），它给神经网络添加了一个非线性函数，从而减少或保持输入层中的通道数量不变，当然如果你愿意，也可以增加通道数量。



**2.6 谷歌 Inception 网络简介（Inception network motivation）**

构建卷积层时，你要决定过滤器的大小究竟是1×1（原来是1×3，猜测为口误），3×3还是5×5，或者要不要添加池化层。而Inception网络的作用就是代替你来决定，虽然网络架构因此变得更加复杂，但网络表现却非常好，我们来了解一下其中的原理。

![img](http://www.ai-start.com/dl2017/images/99f8fc7dbe7cd0726f5271aae11b9872.png)



Inception模块：有1×1的卷积核，也有3×3、5×5的卷积核，并且有最大池化。有了这样的Inception模块，你就可以输入某个量，因为它累加了所有数字，这里的最终输出为32+32+128+64=256。Inception模块的输入为28×28×192，输出为28×28×256。



基本思想是Inception网络不需要人为决定使用哪个过滤器或者是否需要池化，而是由网络自行确定这些参数，你可以给网络添加这些参数的所有可能值，然后把这些输出连接起来，让网络自己学习它需要什么样的参数，采用哪些过滤器组合。

![img](http://www.ai-start.com/dl2017/images/27894eae037f4fd859d33ebdda1cac9a.png)

直接使用5×5的卷积核，需要做120M次乘法。

![img](http://www.ai-start.com/dl2017/images/7d160f6eab22e4b9544b28b44da686a6.png)

而先使用1×1的卷积核，进行缩小，在使用5×5的卷积核进行放大，只需要做2.4M次乘法。



**2.7 Inception 网络（Inception network）**

![img](http://www.ai-start.com/dl2017/images/16a042a0f2d3866909533d409ff2ce3b.png)

最后，将这些方块全都连接起来。在这过程中，把得到的各个层的通道都加起来，最后得到一个28×28×256的输出。通道连接实际就是之前视频中看到过的，把所有方块连接在一起的操作。这就是一个Inception模块，而Inception网络所做的就是将这些模块都组合到一起。

![img](http://www.ai-start.com/dl2017/images/1f2a024a28f664aa704be53cea7ca6f8.png)



**2.9 迁移学习（Transfer Learning）**

这就是卷积网络训练中的迁移学习，事实上，网上的公开数据集非常庞大，并且你下载的其他人已经训练好几周的权重，已经从数据中学习了很多了，你会发现，对于很多计算机视觉的应用，如果你下载其他人的开源的权重，并用作你问题的初始化，你会做的更好。在所有不同学科中，在所有深度学习不同的应用中，我认为计算机视觉是一个你经常用到迁移学习的领域，除非你有非常非常大的数据集，你可以从头开始训练所有的东西。总之，迁移学习是非常值得你考虑的，除非你有一个极其大的数据集和非常大的计算量预算来从头训练你的网络。



**2.10 数据扩充（Data augmentation）**

或许最简单的数据扩充方法就是垂直镜像对称，假如，训练集中有这张图片，然后将其翻转得到右边的图像。对大多数计算机视觉任务，左边的图片是猫，然后镜像对称仍然是猫，如果镜像操作保留了图像中想识别的物体的前提下，这是个很实用的数据扩充技巧。

![img](http://www.ai-start.com/dl2017/images/f92337ae2e50a0896d42d45cc7951e43.png)

另一个经常使用的技巧是随机裁剪，给定一个数据集，然后开始随机裁剪，可能修剪这个（编号1），选择裁剪这个（编号2），这个（编号3），可以得到不同的图片放在数据集中，你的训练集中有不同的裁剪。随机裁剪并不是一个完美的数据扩充的方法，如果你随机裁剪的那一部分（红色方框标记部分，编号4），这部分看起来不像猫。但在实践中，这个方法还是很实用的，随机裁剪构成了很大一部分的真实图片。

![img](http://www.ai-start.com/dl2017/images/709aa552b6a5f4715620047bacf64753.png)

第二种经常使用的方法是彩色转换，有这样一张图片，然后给R、G和B三个通道上加上不同的失真值。

![img](http://www.ai-start.com/dl2017/images/a5bcde6f0d2c2326be700c0ca441c934.png)

在这个例子中（编号1），要给红色、蓝色通道加值，给绿色通道减值。红色和蓝色会产生紫色，使整张图片看起来偏紫，这样训练集中就有失真的图片。为了演示效果，我对图片的颜色进行改变比较夸张。在实践中，对R、G和B的变化是基于某些分布的，这样的改变也可能很小。

2.11 计算机视觉现状（The state of computer vision）