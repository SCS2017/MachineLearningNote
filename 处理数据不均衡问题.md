当你在对一个类别不均衡的数据集进行分类时得到了90%的准确度（Accuracy）。当你进一步分析发现，数据集的90%的样本是属于同一个类，并且分类器将所有的样本都分类为该类。在这种情况下，显然该分类器是无效的。并且这种无效是由于训练集中类别不均衡而导致的。 

  首先举几个所收到的邮件中关于类别不均衡的例子：

- 在一个二分类问题中，训练集中class 1的样本数比class 2的样本数是60:1。使用逻辑回归进行分类，最后结果是其忽略了class 2，即其将所有的训练样本都分类为class 1。
- 在分类任务的数据集中，有三个类别，分别为A，B，C。在训练集中，A类的样本占70%，B类的样本占25%，C类的样本占5%。最后我的分类器对类A的样本过拟合了，而对其它两个类别的样本欠拟合。

什么是类别不均衡问题

  类别数据不均衡是分类任务中一个典型的存在的问题。简而言之，即数据集中，每个类别下的样本数目相差很大。例如，在一个二分类问题中，共有100个样本（100行数据，每一行数据为一个样本的表征），其中80个样本属于class 1，其余的20个样本属于class 2，class 1:class2=80:20=4:1，这便属于类别不均衡。当然，类别不均衡问同样会发生在多分类任务中。它们的解决方法是一样的。因此，为了便于讨论与理解，我们从二分类任务入手进行讲解。

类别不均衡问题是现实中很常见的问题

  大部分分类任务中，各类别下的数据个数基本上不可能完全相等，但是一点点差异是不会产生任何影响与问题的。一般而言，如果类别不平衡比例超过4:1，那么其分类器会大大地因为数据不平衡性而无法满足分类要求的。因此在构建分类模型之前，需要对分类不均衡性问题进行处理。 

  在前面，我们使用准确度这个指标来评价分类质量，可以看出，在类别不均衡时，准确度这个评价指标并不能work。因为分类器将所有的样本都分类到大类下面时，该指标值仍然会很高。即，该分类器偏向了大类这个类别的数据。

八大解决方法

- 可以扩大数据集吗？ 

  当遇到类别不均衡问题时，首先应该想到，是否可能再增加数据（一定要有小类样本数据），更多的数据往往战胜更好的算法。因为机器学习是使用现有的数据多整个数据的分布进行估计，因此更多的数据往往能够得到更多的分布信息，以及更好分布估计。即使再增加小类样本数据时，又增加了大类样本数据，也可以使用放弃一部分大类数据（即对大类数据进行欠采样）来解决。

- 尝试其它评价指标 

  从前面的分析可以看出，准确度这个评价指标在类别不均衡的分类任务中并不能work，甚至进行误导（分类器不work，但是从这个指标来看，该分类器有着很好的评价指标得分）。因此在类别不均衡分类任务中，需要使用更有说服力的评价指标来对分类器进行评价。如何对不同的问题选择有效的评价指标[参见这里](http://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)。 

  上面的超链接中的文章，讲述了如何对乳腺癌患者复发类别不均衡数据进行分类。在文中，推荐了几个比传统的准确度更有效的评价指标：

- - 混淆矩阵(Confusion Matrix)：使用一个表格对分类器所预测的类别与其真实的类别的样本统计，分别为：TP、FN、FP与TN。
  - 精确度(Precision)
  - 召回率(Recall)
  - F1得分(F1 Score)：精确度与找召回率的加权平均。

  特别是：

- - Kappa ([Cohen kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa))
  - ROC曲线(ROC Curves)：见[Assessing and Comparing Classifier Performance with ROC Curves](http://machinelearningmastery.com/assessing-comparing-classifier-performance-roc-curves-2/)

- 对数据集进行重采样 

  可以使用一些策略该减轻数据的不平衡程度。该策略便是采样(sampling)，主要有两种采样方法来降低数据的不平衡性。

- - 对小类的数据样本进行采样来增加小类的数据样本个数，即过采样（over-sampling ，采样的个数大于该类样本的个数）。
  - 对大类的数据样本进行采样来减少该类数据样本的个数，即欠采样（under-sampling，采样的次数少于该类样本的个素）。

  采样算法往往很容易实现，并且其运行速度快，并且效果也不错。更详细的内容参见[这里](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)。 

  一些经验法则：

- - 考虑对大类下的样本（超过1万、十万甚至更多）进行欠采样，即删除部分样本；
  - 考虑对小类下的样本（不足1为甚至更少）进行过采样，即添加部分样本的副本；
  - 考虑尝试随机采样与非随机采样两种采样方法；
  - 考虑对各类别尝试不同的采样比例，比一定是1:1，有时候1:1反而不好，因为与现实情况相差甚远；
  - 考虑同时使用过采样与欠采样。

- 尝试产生人工数据样本 

  一种简单的人工样本数据产生的方法便是，对该类下的所有样本每个属性特征的取值空间中随机选取一个组成新的样本，即属性值随机采样。你可以使用基于经验对属性值进行随机采样而构造新的人工样本，或者使用类似朴素贝叶斯方法假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之前的线性关系（如果本身是存在的）。 

  有一个系统的构造人工数据样本的方法SMOTE(Synthetic Minority Over-sampling Technique)。SMOTE是一种过采样算法，它构造新的小类样本而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。该基于距离度量选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了更多的新生数据。具体可以参见原始论文。 

  这里有SMOTE算法的多个不同语言的实现版本： 

- - Python: [UnbalancedDataset](https://github.com/fmfn/UnbalancedDataset)模块提供了SMOTE算法的多种不同实现版本，以及多种重采样算法。
  - R: [DMwR package](https://blog.csdn.net/heyongluoyao8/article/details/DMwR%20packagehttps://cran.r-project.org/web/packages/DMwR/index.html)。
  - Weka: [SMOTE supervised filter](http://weka.sourceforge.net/doc.packages/SMOTE/weka/filters/supervised/instance/SMOTE.html)。

- 尝试不同的分类算法 

  强烈建议不要对待每一个分类都使用自己喜欢而熟悉的分类算法。应该使用不同的算法对其进行比较，因为不同的算法使用于不同的任务与数据。具体可以参见“Why you should be Spot-Checking Algorithms on your Machine Learning Problems”。 

  决策树往往在类别不均衡数据上表现不错。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开。目前流行的决策树算法有：C4.5、C5.0、CART和Random Forest等。基于R编写的决策树参见[这里](https://blog.csdn.net/heyongluoyao8/article/details/Non-Linear%20Classification%20in%20R%20with%20Decision%20Trees)。基于Python的Scikit-learn的CART使用参见[这里](https://blog.csdn.net/heyongluoyao8/article/details/Get%20Your%20Hands%20Dirty%20With%20Scikit-Learn%20Now)。

- 尝试对模型进行惩罚 

  你可以使用相同的分类算法，但是使用一个不同的角度，比如你的分类任务是识别那些小类，那么可以对分类器的小类样本数据增加权值，降低大类样本的权值（这种方法其实是产生了新的数据分布，即产生了新的数据集，译者注），从而使得分类器将重点集中在小类样本身上。一个具体做法就是，在训练分类器时，若分类器将小类样本分错时额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如penalized-SVM和penalized-LDA算法。 

  Weka中有一个惩罚模型的通用框架[CostSensitiveClassifier](http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/CostSensitiveClassifier.html)，它能够对任何分类器进行封装，并且使用一个自定义的惩罚矩阵对分错的样本进行惩罚。 

  如果你锁定一个具体的算法时，并且无法通过使用重采样来解决不均衡性问题而得到较差的分类结果。这样你便可以使用惩罚模型来解决不平衡性问题。但是，设置惩罚矩阵是一个复杂的事，因此你需要根据你的任务尝试不同的惩罚矩阵，并选取一个较好的惩罚矩阵。

- 尝试一个新的角度理解问题 

  我们可以从不同于分类的角度去解决数据不均衡性问题，我们可以把那些小类的样本作为异常点(outliers)，因此该问题便转化为异常点检测(anomaly detection)与变化趋势检测问题(change detection)。 

  [异常点检测](https://en.wikipedia.org/wiki/Anomaly_detection)即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。 

  [变化趋势检测](https://en.wikipedia.org/wiki/Change_detection)类似于异常点检测，不同在于其通过检测不寻常的变化趋势来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。 

  将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。

- 尝试创新 

  仔细对你的问题进行分析与挖掘，是否可以将你的问题划分成多个更小的问题，而这些小问题更容易解决。你可以从这篇文章[In classification, how do you handle an unbalanced training set?](http://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)中得到灵感。例如： 

- - 将你的大类压缩成小类；
  - 使用One Class分类器（将小类作为异常点）；
  - 使用集成方式，训练多个分类器，然后联合这些分类器进行分类；
  - ….

  这些想法只是冰山一角，你可以想到更多的有趣的和有创意的想法去解决问题。更多的想法参加Reddit的文章[http://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set](https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/)。

选择某一种方法并使用它

  你不必成为一个精通所有算法的算法奇才或者一个建立准确而可靠的处理数据不平衡的模型的统计学家，你只需要根据你的问题的实际情况从上述算法或方法中去选择一种或两种方法去使用。希望上述的某些方法能够解决你的问题。例如使用其它评价指标或重采样算法速度快并且有效。

总结

  记住，其实并不知道哪种方法最适合你的任务与数据，你可以使用一些启发式规则或经验去选择某一个较优算法。当然最好的方法测试每一种算法，然后选择最好的方法。最重要的是，从点滴开始做起，根据自己现有的知识，并不断学习去一步步完善。

Further Reading…

  这里有一些我认为有价值的可供参考的相关资料，让你进一步去认识与研究数据不平衡问题：

- 相关书籍 

- - [Imbalanced Learning: Foundations, Algorithms, and Applications](https://blog.csdn.net/heyongluoyao8/article/details/Imbalanced%20Learning:%20Foundations,%20Algorithms,%20and%20Applications)

- 相关论文 

- - [Data Mining for Imbalanced Datasets: An Overview](https://blog.csdn.net/heyongluoyao8/article/details/Data%20Mining%20for%20Imbalanced%20Datasets:%20An%20Overview)
  - [Learning from Imbalanced Data](http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5128907)
  - [Addressing the Curse of Imbalanced Training Sets: One-Sided Selection (PDF)](http://sci2s.ugr.es/keel/pdf/algorithm/congreso/kubat97addressing.pdf)
  - [A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data](http://dl.acm.org/citation.cfm?id=1007735)